{"cells":[{"cell_type":"code","source":["import os\n\ndbfs_dir = '/databricks-datasets/cs110x/ml-20m/data-001'\nratings_filename = dbfs_dir + '/ratings.csv'\nmovies_filename = dbfs_dir + '/movies.csv'\n\nif os.path.sep != '/':\n  # Handle Windows.\n  ratings_filename = ratings_filename.replace('/', os.path.sep)\n  movie_filename = movie_filename.replace('/', os.path.sep)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nratings_df_schema = StructType(\n  [StructField('userId', IntegerType()),\n   StructField('movieId', IntegerType()),\n   StructField('rating', DoubleType())]\n)\nmovies_df_schema = StructType(\n  [StructField('ID', IntegerType()),\n   StructField('title', StringType())]\n)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_extract\nfrom pyspark.sql.types import *\n\nraw_ratings_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(ratings_df_schema).load(ratings_filename)\nratings_df = raw_ratings_df.drop('Timestamp')\n\nraw_movies_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(movies_df_schema).load(movies_filename)\nmovies_df = raw_movies_df.drop('Genres').withColumnRenamed('movieId', 'ID')\n\nratings_df.cache()\nmovies_df.cache()\n\n\nraw_ratings_count = raw_ratings_df.count()\nratings_count = ratings_df.count()\nraw_movies_count = raw_movies_df.count()\nmovies_count = movies_df.count()\n\nprint 'There are %s ratings and %s movies in the datasets' % (ratings_count, movies_count)\nprint 'Ratings:'\nratings_df.show(3)\nprint 'Movies:'\nmovies_df.show(3, truncate=False)\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["##  Basic Recommendations"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\nmovie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias(\"count\"), F.avg(ratings_df.rating).alias(\"average\"))\nprint 'movie_ids_with_avg_ratings_df:'\nmovie_ids_with_avg_ratings_df.show(3, truncate=False)\n\nmovie_names_df = movie_ids_with_avg_ratings_df.join(movies_df,movie_ids_with_avg_ratings_df.movieId==movies_df.ID)\nmovie_names_with_avg_ratings_df = movie_names_df.select(\"average\",\"title\",\"count\",\"movieId\")\n\nprint 'movie_names_with_avg_ratings_df:'\nmovie_names_with_avg_ratings_df.show(3, truncate=False)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["###  Movies with Highest Average Ratings and at least 500 reviews"],"metadata":{}},{"cell_type":"code","source":["movies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter(\"count >= 500\")\nprint 'Movies with highest ratings:'\nmovies_with_500_ratings_or_more.show(20, truncate=False)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["seed = 1800009193L\n(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([.6,.2,.2],seed)\n\ntraining_df = split_60_df.cache()\nvalidation_df = split_a_20_df.cache()\ntest_df = split_b_20_df.cache()\n\nprint('Training: {0}, validation: {1}, test: {2}\\n'.format(\n  training_df.count(), validation_df.count(), test_df.count())\n)\ntraining_df.show(3)\nvalidation_df.show(3)\ntest_df.show(3)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\nals = ALS()\nals.setMaxIter(5)\\\n   .setSeed(seed)\\\n   .setRegParam(0.1)\\\n   .setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\nfrom pyspark.ml.evaluation import RegressionEvaluator\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n\ntolerance = 0.03\nranks = [4, 8, 12]\nerrors = [0, 0, 0]\nmodels = [0, 0, 0]\nerr = 0\nmin_error = float('inf')\nbest_rank = -1\nfor rank in ranks:\n  als.setRank(rank)\n  model = als.fit(training_df)\n  predict_df = model.transform(validation_df)\n  predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n  error = reg_eval.evaluate(predicted_ratings_df)\n  errors[err] = error\n  models[err] = model\n  print 'For rank %s the RMSE is %s' % (rank, error)\n  if error < min_error:\n    min_error = error\n    best_rank = err\n  err += 1\n\nals.setRank(ranks[best_rank])\nprint 'The best model was trained with rank %s' % ranks[best_rank]\nmy_model = models[best_rank]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### (2c) Testing the Model"],"metadata":{}},{"cell_type":"code","source":["predict_df = my_model.transform(test_df)\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### (2d) Comparing Models"],"metadata":{}},{"cell_type":"code","source":["=avg_rating_df = training_df.agg(F.avg('rating'))\ntraining_avg_rating = avg_rating_df.collect()[0][0]\n\nprint('The average rating for movies in the training set is {0}'.format(training_avg_rating))\ntest_for_avg_df = test_df.withColumn('prediction', F.lit(training_avg_rating))\ntest_avg_RMSE = reg_eval.evaluate(test_for_avg_df)\n\nprint(\"The RMSE on the average set is {0}\".format(test_avg_RMSE))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import Row\nmy_user_id = 0\nmy_rated_movies = [\n     (0,318,5),(0,858,3),(0,50,3),(0,6016,3),(0,2959,3),(0,58559,4),(0,2571,4),(0,4226,4),(0,593,5),(0,296,4)\n     # The format of each line is (my_user_id, movie ID, your rating)\n     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, you would add the following line:\n     #   (my_user_id, 260, 5),\n]\n\nmy_ratings_df = sqlContext.createDataFrame(my_rated_movies, ['userId','movieId','rating'])\nprint 'My movie ratings:'\ndisplay(my_ratings_df.limit(10))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### (3b) Add custom ratings of Movies to Training Dataset"],"metadata":{}},{"cell_type":"code","source":["training_with_my_ratings_df = training_df.unionAll(my_ratings_df)\n\nprint ('The training dataset now has %s more entries than the original training dataset' %\n       (training_with_my_ratings_df.count() - training_df.count()))\nassert (training_with_my_ratings_df.count() - training_df.count()) == my_ratings_df.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["als.setPredictionCol(\"prediction\")\\\n   .setMaxIter(5)\\\n   .setSeed(seed)\\\n   .setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\").setRank(4)\n\nmy_ratings_model = als.fit(training_with_my_ratings_df)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["my_predict_df = my_ratings_model.transform(test_df)\n\npredicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n\ntest_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE_my_ratings))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### (3e) Predict Ratings for new user"],"metadata":{}},{"cell_type":"code","source":["my_rated_movie_ids = [x[1] for x in my_rated_movies]\nnot_rated_df = movies_df.filter(~movies_df[\"ID\"].isin(my_rated_movie_ids))\n\nmy_unrated_movies_df = not_rated_df.select(not_rated_df.ID.alias(\"movieId\"),F.lit(my_user_id).alias(\"userId\"))\n\nraw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n\npredicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction'] != float('nan'))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["predicted_with_counts_df.show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["predicted_with_counts_df = predicted_ratings_df.join(movie_names_with_avg_ratings_df, predicted_ratings_df.movieId == movie_names_with_avg_ratings_df.movieId, \"inner\")\npredicted_highest_rated_movies_df = predicted_with_counts_df.filter(predicted_with_counts_df['count']>75).orderBy(predicted_with_counts_df['prediction'].desc())\n\nprint ('My 25 highest rated movies as predicted (for movies with more than 75 reviews):')\npredicted_highest_rated_movies_df.show(25,truncate=False)"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"cs110_lab2_als_prediction","notebookId":3623611354004766},"nbformat":4,"nbformat_minor":0}
